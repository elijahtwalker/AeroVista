{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "def get_model_instance_segmentation(num_classes, isTrain):\n",
    "    if isTrain:\n",
    "        # load an instance segmentation model pre-trained on COCO\n",
    "        model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=\"DEFAULT\")\n",
    "    else:\n",
    "        # load an instance segmentation model pre-trained on COCO\n",
    "        model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2()\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(\n",
    "        in_features_mask,\n",
    "        hidden_layer,\n",
    "        num_classes\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2 as T\n",
    "def get_transform():\n",
    "    transforms = []\n",
    "    \n",
    "    transforms.append(T.ToDtype(torch.float, scale=True))\n",
    "    transforms.append(T.ColorJitter(brightness=(.1, 1), contrast=(0, 10),\n",
    "                     saturation=(.1, 1), \n",
    "                     hue=(.1, .4)))\n",
    "    transforms.append(T.RandomApply([T.RandomAdjustSharpness(sharpness_factor=10)], p=0.8))\n",
    "    transforms.append(T.RandomApply([T.ElasticTransform(alpha=250)], p=0.5))\n",
    "    transforms.append(T.RandomApply([T.RandomSolarize(threshold=5.0)], p=0.5))\n",
    "    transforms.append(T.RandomApply([T.GaussianBlur(kernel_size=(11, 21), sigma=(5, 50))], p=0.5))\n",
    "    transforms.append(T.ToPureTensor())\n",
    "    \n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "from PIL import Image, ImageDraw\n",
    "from pycocotools.coco import COCO\n",
    "import cv2\n",
    "import json\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "class CurrentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, annotation, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        self.coco = COCO(annotation)\n",
    "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Own coco file\n",
    "        coco = self.coco\n",
    "        # Image ID\n",
    "        img_id = self.ids[index]\n",
    "        # List: get annotation id from coco\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "        # Dictionary: target coco_annotation file for an image\n",
    "        coco_annotation = coco.loadAnns(ann_ids)\n",
    "        # path for input image\n",
    "        path = coco.loadImgs(img_id)[0]['file_name']\n",
    "        # open the input image\n",
    "        img = Image.open(os.path.join(self.root, path))\n",
    "\n",
    "        # number of objects in the image\n",
    "        num_objs = len(coco_annotation)\n",
    "\n",
    "        # Bounding boxes for objects\n",
    "        # In coco format, bbox = [xmin, ymin, width, height]\n",
    "        # In pytorch, the input should be [xmin, ymin, xmax, ymax]\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            xmin = coco_annotation[i]['bbox'][0]\n",
    "            ymin = coco_annotation[i]['bbox'][1]\n",
    "            xmax = xmin + coco_annotation[i]['bbox'][2]\n",
    "            ymax = ymin + coco_annotation[i]['bbox'][3]\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "        \n",
    "        \n",
    "        # Masks\n",
    "        masks = []\n",
    "        \n",
    "        instance_masks = []\n",
    "        #class_ids = []\n",
    "        width, height = img.size\n",
    "        \n",
    "        \n",
    "        for i, annotation in enumerate(coco_annotation):\n",
    "            mask = np.zeros((height, width), dtype=np.uint8)\n",
    "            segments = [[annotation['segmentation'][j], annotation['segmentation'][j+1]] for j in range(0, len(annotation['segmentation']), 2)]\n",
    "            cv2.fillPoly(mask, [np.array(segments)], color=(255, 255, 255))\n",
    "        \n",
    "            mask = mask.astype(np.float32) / 255.0\n",
    "            mask = mask.astype(np.uint8)\n",
    "            instance_masks.append(mask)\n",
    "                \n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        masks = torch.as_tensor(instance_masks, dtype=torch.uint8)\n",
    "\n",
    "        # Labels (In my case, I only one class: target class or background)\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        boxes = torch.tensor(boxes)\n",
    "        labels = torch.tensor(labels)\n",
    "        masks = torch.tensor(masks)\n",
    "\n",
    "        return torch.from_numpy(np.array(img).astype(dtype=np.float32)), boxes, labels, masks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def collate_fn(batch):\n",
    "    imgs, box, label, mask = zip(*batch)\n",
    "\n",
    "    boxes = pad_sequence(box, batch_first=True, padding_value=0)\n",
    "    num_rows = len(boxes)\n",
    "    \n",
    "    for i in range(num_rows):\n",
    "        pad_len = len(boxes[i])\n",
    "        for j in range(pad_len):\n",
    "            a = boxes[i][j][0]\n",
    "            b = boxes[i][j][1]\n",
    "            c = boxes[i][j][2]\n",
    "            d = boxes[i][j][3] \n",
    "            \n",
    "            if a == 0 and b == 0 and c == 0 and d == 0:\n",
    "                boxes[i][j][2] = 1e-9\n",
    "                boxes[i][j][3] = 1e-9\n",
    "    \n",
    "    labels = pad_sequence(label, batch_first=True, padding_value=0)\n",
    "    masks = pad_sequence(mask, batch_first=True, padding_value=0)\n",
    "    \n",
    "    res_img = []\n",
    "    for im in imgs:\n",
    "        res_img.append(im.tolist())\n",
    "\n",
    "    res = []\n",
    "    for i, (b, l, m) in enumerate(zip(boxes, labels, masks)):\n",
    "        annotations = {}\n",
    "        annotations['boxes'] = b\n",
    "        annotations['labels'] = l\n",
    "        annotations['masks'] = m\n",
    "        \n",
    "        res.append(annotations)\n",
    "\n",
    "    return torch.tensor(res_img), res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fnValid(batch):\n",
    "    imgs, box, label, mask = zip(*batch)\n",
    "\n",
    "    boxes = pad_sequence(box, batch_first=True, padding_value=0)\n",
    "    num_rows = len(boxes)\n",
    "    \n",
    "    for i in range(num_rows):\n",
    "        pad_len = len(boxes[i])\n",
    "        for j in range(pad_len):\n",
    "            a = boxes[i][j][0]\n",
    "            b = boxes[i][j][1]\n",
    "            c = boxes[i][j][2]\n",
    "            d = boxes[i][j][3] \n",
    "            \n",
    "            if a == 0 and b == 0 and c == 0 and d == 0:\n",
    "                boxes[i][j][2] = 1e-9\n",
    "                boxes[i][j][3] = 1e-9\n",
    "    \n",
    "    labels = pad_sequence(label, batch_first=True, padding_value=0)\n",
    "    masks = pad_sequence(mask, batch_first=True, padding_value=0)\n",
    "    \n",
    "    scores_values = [1.0, 1.0, 1.0, 1.0]\n",
    "    scores = torch.tensor(scores_values)\n",
    "    \n",
    "    res_img = []\n",
    "    for im in imgs:\n",
    "        res_img.append(im.tolist())\n",
    "\n",
    "    res = []\n",
    "    for i, (b, l, m) in enumerate(zip(boxes, labels, masks)):\n",
    "        annotations = {}\n",
    "        annotations['boxes'] = b\n",
    "        annotations['labels'] = l\n",
    "        annotations['masks'] = m\n",
    "        annotations['scores'] = scores\n",
    "        \n",
    "        res.append(annotations)\n",
    "\n",
    "    return torch.tensor(res_img), res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from currentdataset import CurrentDataset\n",
    "import numpy as np\n",
    "# path to your own data and coco file\n",
    "train_data_dir = '../data/sard_yolo/images/train'\n",
    "train_coco = '../data/sard_yolo/ann_files/binary_masks/_train_annotations.coco.json'\n",
    "\n",
    "valid_data_dir = '../data/sard_yolo/images/valid'\n",
    "valid_coco = '../data/sard_yolo/ann_files/binary_masks/_valid_annotations.coco.json'\n",
    "\n",
    "# create own Dataset\n",
    "train_dataset = CurrentDataset(root=train_data_dir,\n",
    "                          annotation=train_coco,\n",
    "                          transforms = get_transform\n",
    "                          )\n",
    "\n",
    "valid_dataset = CurrentDataset(root=valid_data_dir,\n",
    "                          annotation=valid_coco,\n",
    "                          )\n",
    "\n",
    "# Batch sizes\n",
    "train_batch_size = 8\n",
    "valid_batch_size = 4\n",
    "\n",
    "# own DataLoader\n",
    "trainData_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                          batch_size=train_batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=2,\n",
    "                                          collate_fn=collate_fn\n",
    "                                        )\n",
    "validData_loader = torch.utils.data.DataLoader(valid_dataset,\n",
    "                                          batch_size=train_batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=2,\n",
    "                                          collate_fn=collate_fnValid\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    # Unpack the coordinates for easier reading\n",
    "    x1 = box1[0][0]\n",
    "    y1 = box1[0][1]\n",
    "    w1 = box1[0][2]\n",
    "    h1 = box1[0][3]\n",
    "\n",
    "    x2 = box2[0][0]\n",
    "    y2 = box2[0][1]\n",
    "    w2 = box2[0][2]\n",
    "    h2 = box2[0][3]\n",
    "\n",
    "    # Calculate the coordinates of the intersection rectangle\n",
    "    x_left = max(x1, x2)\n",
    "    y_top = max(y1, y2)\n",
    "    x_right = min(x1 + w1, x2 + w2)\n",
    "    y_bottom = min(y1 + h1, y2 + h2)\n",
    "\n",
    "    # Calculate area of intersection rectangle\n",
    "    if x_right < x_left or y_bottom < y_top:\n",
    "        return 0.0\n",
    "    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n",
    "\n",
    "    # Calculate area of both bounding boxes\n",
    "    box1_area = w1 * h1\n",
    "    box2_area = w2 * h2\n",
    "\n",
    "    # Calculate union area by using inclusion-exclusion principle\n",
    "    union_area = box1_area + box2_area - intersection_area\n",
    "\n",
    "    # Compute the IoU\n",
    "    iou = intersection_area / union_area\n",
    "    return iou.item()\n",
    "\n",
    "def calculate_boxApAr(detections, annotations, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Calculate Average Precision (AP) for bounding boxes.\n",
    "\n",
    "    Args:\n",
    "        pred_bboxes (List[Tensor]): List of predicted bounding boxes.\n",
    "        gt_bboxes (List[Tensor]): List of ground truth bounding boxes.\n",
    "        confidences (List[float]): List of confidence scores for predicted bounding boxes.\n",
    "        iou_threshold (float): IoU threshold to consider a detection as correct.\n",
    "\n",
    "    Returns:\n",
    "        float: Average Precision (AP) value.\n",
    "    \"\"\"\n",
    "    # Sort predicted bounding boxes by confidence scores in descending order\n",
    "    # sorted_indices = np.argsort(confidences)[::-1]\n",
    "    \n",
    "    # detections = [detections[i]['boxes'] for i in sorted_indices]\n",
    "\n",
    "    true_positives = np.zeros(len(detections))\n",
    "    false_positives = np.zeros(len(detections))\n",
    "    num_gt_bboxes = len(annotations)\n",
    "    used_gt_bboxes = np.zeros(num_gt_bboxes)\n",
    "\n",
    "    # Calculate IoU for each predicted bounding box\n",
    "    for i in range(0, len(detections)):\n",
    "        max_iou = -1\n",
    "        max_iou_idx = -1\n",
    "        for j in range(0, len(annotations)):\n",
    "            iou = calculate_iou(detections[i]['boxes'], annotations[j]['boxes'])\n",
    "            if iou > max_iou:\n",
    "                max_iou = iou\n",
    "                max_iou_idx = j\n",
    "        \n",
    "        #print(f\"MAX IOU: {max_iou}\")\n",
    "        if max_iou >= iou_threshold:\n",
    "            if not used_gt_bboxes[max_iou_idx]:\n",
    "                true_positives[i] = 1\n",
    "                used_gt_bboxes[max_iou_idx] = 1\n",
    "            else:\n",
    "                false_positives[i] = 1\n",
    "        else:\n",
    "            false_positives[i] = 1\n",
    "\n",
    "    # Compute precision and recall\n",
    "    cum_tp = np.cumsum(true_positives)\n",
    "    cum_fp = np.cumsum(false_positives)\n",
    "    precision = cum_tp / (cum_tp + cum_fp)\n",
    "    recall = cum_tp / num_gt_bboxes\n",
    "\n",
    "    ar = np.mean(recall)\n",
    "\n",
    "    # Compute Average Precision (AP) using the precision-recall curve\n",
    "    ap = 0\n",
    "    for i in range(1, len(precision)):\n",
    "        ap += (recall[i] - recall[i - 1]) * precision[i]\n",
    "    ar = np.sum(np.diff(recall)) / len(recall)\n",
    "    return ap, ar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "\n",
    "plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
    "\n",
    "\n",
    "def show(imgs):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = F.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "        \n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.io import read_image\n",
    "from pathlib import Path\n",
    "from torchvision.utils import draw_segmentation_masks\n",
    "\n",
    "def makingMaskForData(img, data):\n",
    "    data_int = read_image(str(img))\n",
    "    proba_threshold = 0.95\n",
    "    data_bool_masks = data['scores']> proba_threshold\n",
    "    data_bool_masks = (data['masks'].squeeze(1)).bool()\n",
    "    \n",
    "    show(draw_segmentation_masks(data_int, data_bool_masks, alpha=0.9))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Dict, Optional\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from collections import OrderedDict\n",
    "from torchvision.models.detection.roi_heads import fastrcnn_loss\n",
    "from torchvision.models.detection.rpn import concat_box_prediction_layers\n",
    "def eval_forward(model, images, targets):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        images (list[Tensor]): images to be processed\n",
    "        targets (list[Dict[str, Tensor]]): ground-truth boxes present in the image (optional)\n",
    "    Returns:\n",
    "        result (list[BoxList] or dict[Tensor]): the output from the model.\n",
    "            It returns list[BoxList] contains additional fields\n",
    "            like `scores`, `labels` and `mask` (for Mask R-CNN models).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    original_image_sizes: List[Tuple[int, int]] = []\n",
    "    for img in images:\n",
    "        val = img.shape[-2:]\n",
    "        assert len(val) == 2\n",
    "        original_image_sizes.append((val[0], val[1]))\n",
    "\n",
    "    images, targets = model.transform(images, targets)\n",
    "\n",
    "    # Check for degenerate boxes\n",
    "    # TODO: Move this to a function\n",
    "    if targets is not None:\n",
    "        for target_idx, target in enumerate(targets):\n",
    "            boxes = target[\"boxes\"]\n",
    "            degenerate_boxes = boxes[:, 2:] <= boxes[:, :2]\n",
    "            if degenerate_boxes.any():\n",
    "                # print the first degenerate box\n",
    "                bb_idx = torch.where(degenerate_boxes.any(dim=1))[0][0]\n",
    "                degen_bb: List[float] = boxes[bb_idx].tolist()\n",
    "                raise ValueError(\n",
    "                    \"All bounding boxes should have positive height and width.\"\n",
    "                    f\" Found invalid box {degen_bb} for target at index {target_idx}.\"\n",
    "                )\n",
    "\n",
    "    features = model.backbone(images.tensors)\n",
    "    if isinstance(features, torch.Tensor):\n",
    "        features = OrderedDict([(\"0\", features)])\n",
    "    model.rpn.training=True\n",
    "    #model.roi_heads.training=True\n",
    "\n",
    "\n",
    "    #####proposals, proposal_losses = model.rpn(images, features, targets)\n",
    "    features_rpn = list(features.values())\n",
    "    objectness, pred_bbox_deltas = model.rpn.head(features_rpn)\n",
    "    anchors = model.rpn.anchor_generator(images, features_rpn)\n",
    "\n",
    "    num_images = len(anchors)\n",
    "    num_anchors_per_level_shape_tensors = [o[0].shape for o in objectness]\n",
    "    num_anchors_per_level = [s[0] * s[1] * s[2] for s in num_anchors_per_level_shape_tensors]\n",
    "    objectness, pred_bbox_deltas = concat_box_prediction_layers(objectness, pred_bbox_deltas)\n",
    "    # apply pred_bbox_deltas to anchors to obtain the decoded proposals\n",
    "    # note that we detach the deltas because Faster R-CNN do not backprop through\n",
    "    # the proposals\n",
    "    proposals = model.rpn.box_coder.decode(pred_bbox_deltas.detach(), anchors)\n",
    "    proposals = proposals.view(num_images, -1, 4)\n",
    "    proposals, scores = model.rpn.filter_proposals(proposals, objectness, images.image_sizes, num_anchors_per_level)\n",
    "\n",
    "    proposal_losses = {}\n",
    "    assert targets is not None\n",
    "    labels, matched_gt_boxes = model.rpn.assign_targets_to_anchors(anchors, targets)\n",
    "    regression_targets = model.rpn.box_coder.encode(matched_gt_boxes, anchors)\n",
    "    loss_objectness, loss_rpn_box_reg = model.rpn.compute_loss(\n",
    "        objectness, pred_bbox_deltas, labels, regression_targets\n",
    "    )\n",
    "    proposal_losses = {\n",
    "        \"loss_objectness\": loss_objectness,\n",
    "        \"loss_rpn_box_reg\": loss_rpn_box_reg,\n",
    "    }\n",
    "\n",
    "    #####detections, detector_losses = model.roi_heads(features, proposals, images.image_sizes, targets)\n",
    "    image_shapes = images.image_sizes\n",
    "    proposals, matched_idxs, labels, regression_targets = model.roi_heads.select_training_samples(proposals, targets)\n",
    "    box_features = model.roi_heads.box_roi_pool(features, proposals, image_shapes)\n",
    "    box_features = model.roi_heads.box_head(box_features)\n",
    "    class_logits, box_regression = model.roi_heads.box_predictor(box_features)\n",
    "\n",
    "    result: List[Dict[str, torch.Tensor]] = []\n",
    "    detector_losses = {}\n",
    "    loss_classifier, loss_box_reg = fastrcnn_loss(class_logits, box_regression, labels, regression_targets)\n",
    "    detector_losses = {\"loss_classifier\": loss_classifier, \"loss_box_reg\": loss_box_reg}\n",
    "    boxes, scores, labels = model.roi_heads.postprocess_detections(class_logits, box_regression, proposals, image_shapes)\n",
    "    num_images = len(boxes)\n",
    "    for i in range(num_images):\n",
    "        result.append(\n",
    "            {\n",
    "                \"boxes\": boxes[i],\n",
    "                \"labels\": labels[i],\n",
    "                \"scores\": scores[i],\n",
    "            }\n",
    "        )\n",
    "    detections = result\n",
    "    detections = model.transform.postprocess(detections, images.image_sizes, original_image_sizes)  # type: ignore[operator]\n",
    "    model.rpn.training=False\n",
    "    model.roi_heads.training=False\n",
    "    losses = {}\n",
    "    losses.update(detector_losses)\n",
    "    losses.update(proposal_losses)\n",
    "    return losses, detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 classes; Only target class or background\n",
    "num_classes = 2\n",
    "# It is pretrained on coco so it should not need to be \n",
    "num_epochs = 25\n",
    "model = get_model_instance_segmentation(num_classes, True)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# parameters\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "# Try LR of 0.001\n",
    "optimizer = torch.optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0001)\n",
    "\n",
    "len_trainDataloader = len(trainData_loader)\n",
    "len_validDataloader = len(validData_loader)\n",
    "\n",
    "training_loss = []\n",
    "valid_losses = []\n",
    "apAll = []\n",
    "arAll = []\n",
    "mAPAll = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for i, (imgs, annotations) in enumerate(trainData_loader, 1):\n",
    "        i += 1\n",
    "        imgs = torch.tensor(imgs).to(device)\n",
    "        imgs = torch.permute(imgs, (0, 3, 1, 2))\n",
    "\n",
    "        for ann in annotations:\n",
    "            ann['boxes'] = ann['boxes'].to(device)\n",
    "            ann['masks'] = ann['masks'].to(device)\n",
    "            ann['labels'] = ann['labels'].to(device)\n",
    "        \n",
    "        loss_dict = model(imgs, annotations)\n",
    "        losses = sum(loss for loss in loss_dict.values())/train_batch_size\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        training_loss.append(losses.detach().cpu().item())\n",
    "        if i % 29 == 0 : print(f'TRAIN -- Iteration: {i}/{len_trainDataloader}, Loss: {losses}')\n",
    "\n",
    "    print(f'TRAINING -- Epoch: {epoch}, Loss: {sum(training_loss)/len_trainDataloader}')\n",
    "        \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for j, (imgs, annotations) in enumerate(validData_loader, 1): \n",
    "            j += 1\n",
    "            val_loss = 0\n",
    "            imgs = torch.tensor(imgs).to(device)\n",
    "            imgs = torch.permute(imgs, (0, 3, 1, 2))\n",
    "            for ann in annotations:\n",
    "                ann['boxes'] = ann['boxes'].to(device)\n",
    "                ann['masks'] = ann['masks'].to(device)\n",
    "                ann['labels'] = ann['labels'].to(device)\n",
    "                ann['scores'] = ann['scores'].to(device)\n",
    "            \n",
    "            losses, detections = eval_forward(model, imgs, annotations)\n",
    "            val_loss += (sum(loss for loss in losses.values())/valid_batch_size)\n",
    "            valid_losses.append(val_loss.detach().cpu().item())\n",
    "            ap, ar = calculate_boxApAr(detections, annotations)\n",
    "            apAll.append(ap)\n",
    "            arAll.append(ar)\n",
    "            mAPAll.append((sum(apAll)/len(apAll)))\n",
    "\n",
    "            if j % 10 == 0 : print(f'VALIDATION -- Iteration: {j}/{len_validDataloader}, Loss: {val_loss}')\n",
    "        \n",
    "    print(f'VALIDATION -- Epoch: {epoch}, Loss: {sum(valid_losses)/len_validDataloader}')\n",
    "\n",
    "    model_dir = 'checkpoints'\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    torch.save(model.state_dict(), f'{model_dir}/ckpt-model-{epoch+1}.pt')\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(mAPAll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg_ap = []\n",
    "# for i in range(0, len(training_loss), 50):\n",
    "#     subset = apAll[i:i+50]\n",
    "#     avg = np.mean(subset)\n",
    "#     avg_ap.append(avg)\n",
    "\n",
    "bins_mAP = np.linspace(0, num_epochs, len(mAPAll))\n",
    "#bins_ap = np.linspace(0, num_epochs, len(avg_ap))\n",
    "\n",
    "#plt.plot(bins_ap, avg_ap, color='steelblue', label='ap overall')\n",
    "plt.plot(bins_mAP, mAPAll, color='darkblue', label='mask mAP at 50')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('mAP')\n",
    "plt.title('Segmentation mAP')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_train = []\n",
    "avg_valid = []\n",
    "window_train = 2\n",
    "window_valid = 3\n",
    "\n",
    "print(len(training_loss))\n",
    "print(len(valid_losses))\n",
    "\n",
    "for i in range(0, len(training_loss), window_train):\n",
    "    subset = training_loss[i:i+window_train]\n",
    "    avg = np.mean(subset)\n",
    "    avg_train.append(avg)\n",
    "\n",
    "for i in range(0, len(valid_losses), window_valid):\n",
    "    subset = valid_losses[i:i+window_valid]\n",
    "    avg = np.mean(subset)\n",
    "    avg_valid.append(avg)\n",
    "\n",
    "# print(avg_train)\n",
    "# print(avg_valid)\n",
    "\n",
    "bins_train = np.linspace(0, num_epochs, len(avg_train))\n",
    "bins_valid = np.linspace(0, num_epochs, len(avg_valid))\n",
    "\n",
    "plt.plot(bins_train, avg_train, color='darkblue', label='Training Loss')\n",
    "plt.plot(bins_valid, avg_valid, color='steelblue', label='Validation')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Functions for Training and Validation')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "from coco_eval import CocoEvaluator\n",
    "import utils\n",
    "import coco_utils\n",
    "\n",
    "model = get_model_instance_segmentation(2, False).to('cuda')\n",
    "model.load_state_dict(torch.load('checkpoints/ckpt-model-10.pt'))\n",
    "\n",
    "test_dataset = CurrentDataset(root='../data/sard_yolo/images/test',\n",
    "                          annotation='../data/sard_yolo/ann_files/binary_masks/_test_annotations.coco.json',\n",
    "                          )\n",
    "\n",
    "# Batch sizes\n",
    "test_batch_size = 4\n",
    "\n",
    "# own DataLoader\n",
    "testData_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                          batch_size=test_batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=2,\n",
    "                                          collate_fn=collate_fnValid\n",
    "                                          )\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    # Unpack the coordinates for easier reading\n",
    "    x1 = box1[0][0]\n",
    "    y1 = box1[0][1]\n",
    "    w1 = box1[0][2]\n",
    "    h1 = box1[0][3]\n",
    "\n",
    "    x2 = box2[0][0]\n",
    "    y2 = box2[0][1]\n",
    "    w2 = box2[0][2]\n",
    "    h2 = box2[0][3]\n",
    "\n",
    "    # Calculate the coordinates of the intersection rectangle\n",
    "    x_left = max(x1, x2)\n",
    "    y_top = max(y1, y2)\n",
    "    x_right = min(x1 + w1, x2 + w2)\n",
    "    y_bottom = min(y1 + h1, y2 + h2)\n",
    "\n",
    "    # Calculate area of intersection rectangle\n",
    "    if x_right < x_left or y_bottom < y_top:\n",
    "        return 0.0\n",
    "    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n",
    "\n",
    "    # Calculate area of both bounding boxes\n",
    "    box1_area = w1 * h1\n",
    "    box2_area = w2 * h2\n",
    "\n",
    "    # Calculate union area by using inclusion-exclusion principle\n",
    "    union_area = box1_area + box2_area - intersection_area\n",
    "\n",
    "    # Compute the IoU\n",
    "    iou = intersection_area / union_area\n",
    "    return iou.item()\n",
    "\n",
    "def calculate_boxApAr(detections, annotations, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Calculate Average Precision (AP) for bounding boxes.\n",
    "\n",
    "    Args:\n",
    "        pred_bboxes (List[Tensor]): List of predicted bounding boxes.\n",
    "        gt_bboxes (List[Tensor]): List of ground truth bounding boxes.\n",
    "        confidences (List[float]): List of confidence scores for predicted bounding boxes.\n",
    "        iou_threshold (float): IoU threshold to consider a detection as correct.\n",
    "\n",
    "    Returns:\n",
    "        float: Average Precision (AP) value.\n",
    "    \"\"\"\n",
    "    # Sort predicted bounding boxes by confidence scores in descending order\n",
    "    # sorted_indices = np.argsort(confidences)[::-1]\n",
    "    \n",
    "    # detections = [detections[i]['boxes'] for i in sorted_indices]\n",
    "\n",
    "    true_positives = np.zeros(len(detections))\n",
    "    false_positives = np.zeros(len(detections))\n",
    "    num_gt_bboxes = len(annotations)\n",
    "    used_gt_bboxes = np.zeros(num_gt_bboxes)\n",
    "\n",
    "    # Calculate IoU for each predicted bounding box\n",
    "    for i in range(0, len(detections)):\n",
    "        max_iou = -1\n",
    "        max_iou_idx = -1\n",
    "        for j in range(0, len(annotations)):\n",
    "            iou = calculate_iou(detections[i]['boxes'], annotations[j]['boxes'])\n",
    "            if iou > max_iou:\n",
    "                max_iou = iou\n",
    "                max_iou_idx = j\n",
    "        \n",
    "        #print(f\"MAX IOU: {max_iou}\")\n",
    "        if max_iou >= iou_threshold:\n",
    "            if not used_gt_bboxes[max_iou_idx]:\n",
    "                true_positives[i] = 1\n",
    "                used_gt_bboxes[max_iou_idx] = 1\n",
    "            else:\n",
    "                false_positives[i] = 1\n",
    "        else:\n",
    "            false_positives[i] = 1\n",
    "\n",
    "    # Compute precision and recall\n",
    "    cum_tp = np.cumsum(true_positives)\n",
    "    cum_fp = np.cumsum(false_positives)\n",
    "    precision = cum_tp / (cum_tp + cum_fp)\n",
    "    recall = cum_tp / num_gt_bboxes\n",
    "\n",
    "    ar = np.mean(recall)\n",
    "\n",
    "    # Compute Average Precision (AP) using the precision-recall curve\n",
    "    ap = 0\n",
    "    for i in range(1, len(precision)):\n",
    "        ap += (recall[i] - recall[i - 1]) * precision[i]\n",
    "    ar = np.sum(np.diff(recall)) / len(recall)\n",
    "    return ap, ar\n",
    "\n",
    "def calculate_maskApAr(detections, annotations, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Calculate Average Precision (AP) and Average Recall (AR) for masks.\n",
    "\n",
    "    Args:\n",
    "        detections (List[Dict]): List of dictionaries containing predicted masks.\n",
    "        annotations (List[Dict]): List of dictionaries containing ground truth masks.\n",
    "        iou_threshold (float): IoU threshold to consider a detection as correct.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: Tuple containing Average Precision (AP) and Average Recall (AR) values.\n",
    "    \"\"\"\n",
    "    true_positives = np.zeros(len(detections))\n",
    "    false_positives = np.zeros(len(detections))\n",
    "    num_gt_masks = len(annotations)\n",
    "    used_gt_masks = np.zeros(num_gt_masks)\n",
    "\n",
    "    # Calculate IoU for each predicted mask\n",
    "    for i in range(len(detections)):\n",
    "        if (detections[0]['masks'][i] == 1 and annotations[0]['masks'][i] == 1):\n",
    "            true_positives[i] = 1\n",
    "        elif (detections[0]['masks'][i] == 1 and annotations[0]['masks'][i] != 1): \n",
    "            false_positives[i] = 1\n",
    "\n",
    "    # Compute precision and recall\n",
    "    cum_tp = np.cumsum(true_positives)\n",
    "    cum_fp = np.cumsum(false_positives)\n",
    "    precision = cum_tp / (cum_tp + cum_fp)\n",
    "    recall = cum_tp / num_gt_masks\n",
    "\n",
    "    # Compute Average Precision (AP) using the precision-recall curve\n",
    "    ap = 0\n",
    "    for i in range(1, len(precision)):\n",
    "        ap += (recall[i] - recall[i - 1]) * precision[i]\n",
    "\n",
    "    # Compute Average Recall (AR)\n",
    "    ar = np.sum(np.diff(recall)) / len(recall)\n",
    "\n",
    "    return ap, ar\n",
    "\n",
    "# Helper function to calculate IoU between masks\n",
    "def calculate_mask_iou(mask1, mask2):\n",
    "    intersection = np.logical_and(mask1, mask2)\n",
    "    union = np.logical_or(mask1, mask2)\n",
    "    iou = np.sum(intersection) / np.sum(union)\n",
    "    return iou\n",
    "\n",
    "\n",
    "num_epochs_test = 2\n",
    "ap_all = []\n",
    "mAP_all = []\n",
    "ar_all = []\n",
    "\n",
    "maskAp_all = []\n",
    "maskAr_all = []\n",
    "maskmAP_all = []\n",
    "\n",
    "model.eval()\n",
    "for epoch in range(num_epochs_test):\n",
    "    with torch.no_grad():\n",
    "        for j, (imgs, annotations) in enumerate(testData_loader, 1): \n",
    "            j += 1\n",
    "            val_loss = 0\n",
    "            imgs = torch.tensor(imgs).to(device)\n",
    "            imgs = torch.permute(imgs, (0, 3, 1, 2))\n",
    "            for ann in annotations:\n",
    "                ann['boxes'] = ann['boxes'].to(device)\n",
    "                ann['masks'] = ann['masks'].to(device)\n",
    "                ann['labels'] = ann['labels'].to(device)\n",
    "                ann['scores'] = ann['scores'].to(device)\n",
    "            \n",
    "            losses, detections = eval_forward(model, imgs, annotations)\n",
    "            val_loss += (sum(loss for loss in losses.values())/valid_batch_size)\n",
    "            valid_losses.append(val_loss.detach().cpu().item())\n",
    "            #print(detections[0]['boxes'])\n",
    "            #print(annotations[0]['boxes'])\n",
    "            #print(detections)\n",
    "            ap, ar = calculate_boxApAr(detections, annotations)\n",
    "            #maskAp, maskAr = calculate_maskApAr(detections, annotations)\n",
    "            ap_all.append(ap)\n",
    "            mAP_all.append(sum(ap_all)/len(ap_all))\n",
    "            ar_all.append(ar)\n",
    "\n",
    "            # maskAp_all.append(maskAp)\n",
    "            # maskAr_all.append(maskAr)\n",
    "            # maskmAP_all.append(sum(maskAp_all)/len(maskAp_all))\n",
    "\n",
    "            \n",
    "            if j%10 == 0: print(f'TEST -- Iteration: {j}/{len(testData_loader)}, Loss: {val_loss}')\n",
    "    \n",
    "print(f'TEST -- Epoch: {epoch}, Loss: {sum(valid_losses)/len(testData_loader)}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
